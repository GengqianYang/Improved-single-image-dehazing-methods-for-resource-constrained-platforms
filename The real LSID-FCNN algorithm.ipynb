{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import torchvision\n",
    "import torch.optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network architecture\n",
    "class network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(network, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv3 = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv4 = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv5 = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "        self.conv6 = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=7, stride=1, padding=3, bias=True)\n",
    "        self.conv7 = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=9, stride=1, padding=4, bias=True)\n",
    "        self.conv8 = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv9 = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv10 = nn.Conv2d(in_channels=27, out_channels=3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        x1 = self.relu(self.conv1(x))\n",
    "        x2 = self.relu(self.conv2(x1))\n",
    "\n",
    "        concat1 = torch.cat((x1,x2), 1)\n",
    "        x3 = self.relu(self.conv3(concat1))\n",
    "\n",
    "        concat2 = torch.cat((x2,x3), 1)\n",
    "        x4 = self.relu(self.conv4(concat2))\n",
    "\n",
    "        concat3 = torch.cat((x3,x4),1)\n",
    "        x5 = self.relu(self.conv5(concat3))\n",
    "        \n",
    "        concat4 = torch.cat((x4,x5),1)\n",
    "        x6 = self.relu(self.conv6(concat4))\n",
    "        \n",
    "        concat5 = torch.cat((x5,x6),1)\n",
    "        x7 = self.relu(self.conv7(concat5))\n",
    "        \n",
    "        concat6 = torch.cat((x6,x7),1)\n",
    "        x8 = self.relu(self.conv8(concat6))\n",
    "        \n",
    "        concat7 = torch.cat((x7,x8),1)\n",
    "        x9 = self.relu(self.conv9(concat7))\n",
    "        \n",
    "        concat8 = torch.cat((x1,x2,x3,x4,x5,x6,x7,x8,x9),1)\n",
    "        x10 = self.relu(self.conv10(concat8))\n",
    "\n",
    "        clean_image = self.relu((x10 * x) - x10 + 1) \n",
    "\n",
    "        return clean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network(\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (conv1): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(6, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv6): Conv2d(6, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "  (conv7): Conv2d(6, 3, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
      "  (conv8): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv9): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv10): Conv2d(27, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#check the network architecture\n",
    "print(network())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "network(\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (conv1): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5): Conv2d(6, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv6): Conv2d(6, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (conv7): Conv2d(6, 3, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "  (conv8): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv9): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv10): Conv2d(27, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize the network\n",
    "def initialization(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)   \n",
    "\n",
    "#check if the GPU accleration is available (training without GPU accleration can be extremely slow)\n",
    "print(torch.cuda.is_available())\n",
    "#from CPU to GPU\n",
    "net=network()\n",
    "#net = network().cuda()\n",
    "net.apply(initialization)\n",
    "#check the weight\n",
    "#net.conv1.weight           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please change the name of the root according to where your training dataset is.\n",
    "#You need one train.txt file under the root (for example:C:\\\\Users\\\\Gengqian Yang\\\\dataset\\\\VOC2012\\\\ImageSets\\\\Segmentation\\\\ ) \n",
    "#The train.txt file should include the name for all training examples\n",
    "#The training dataset can be found at Github, you only need to change the name of the root if you already downloaded it.\n",
    "def read_voc_images(root=\"C:\\\\Users\\\\Gengqian Yang\\\\dataset\\\\VOC2012\", \n",
    "                    is_train=True, max_num=None):\n",
    "    txt_fname = '%s/ImageSets/Segmentation/%s' % (\n",
    "        root, 'train.txt' if is_train else 'val.txt')\n",
    "    with open(txt_fname, 'r') as f:\n",
    "        images_fullnames = f.read().split() #images is the list of all images\n",
    "        images_v1=[]\n",
    "        images=[]\n",
    "        labels_names=[]\n",
    "        for i in range(len(images_fullnames)):\n",
    "            if i%2==1:\n",
    "                images_v1.append(images_fullnames[i])\n",
    "        for image_fullname in images_v1:\n",
    "            images.append(image_fullname.split('\\\\')[-1])\n",
    "        for image in images:\n",
    "            labels_names.append(image.split(\"_\")[0])\n",
    "    if max_num is not None:\n",
    "        images = images[:min(max_num, len(images))]\n",
    "        labels_names = labels_names[:min(max_num, len(labels_names))]\n",
    "    features, labels = [None] * len(images), [None] * len(labels_names) # features and labels are empty lists\n",
    "    for i, fname in tqdm(enumerate(images)):\n",
    "        features[i] = Image.open('%s\\\\JPEGImages\\\\%s' % (root, fname)).convert(\"RGB\") #feature is the list of input images\n",
    "    for i, fname in tqdm(enumerate(labels_names)):\n",
    "        labels[i] = Image.open('%s\\\\SegmentationClass\\\\%s.png' % (root, fname)).convert(\"RGB\")#label is the list of ground truth images\n",
    "    return features, labels # PIL images list\n",
    "\n",
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, is_train, voc_dir, max_num=None):\n",
    "        #data standardisation\n",
    "        self.rgb_mean = np.array([0.485, 0.456, 0.406]) #mean\n",
    "        self.rgb_std = np.array([0.229, 0.224, 0.225]) #standard deviation\n",
    "        self.transform=torchvision.transforms.ToTensor()\n",
    "        self.tsf = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean=self.rgb_mean, \n",
    "                                             std=self.rgb_std)\n",
    "        ]) #transform the input list into a tensor then apply the standardisation\n",
    "        \n",
    "        features, labels = read_voc_images(root=voc_dir, \n",
    "                                           is_train=is_train, \n",
    "                                           max_num=max_num) # load features and labels\n",
    "        self.features = features   \n",
    "        self.labels = labels\n",
    "        print('read ' + str(len(self.features)) + ' valid examples') \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        feature, label = self.features[idx], self.labels[idx]\n",
    "                                       #randomly choose corresponding feature and label of the index\n",
    "        \n",
    "        return self.transform(feature),self.transform(label)\n",
    "                #standardisation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) #return the length of dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please change the name of the root according to where your training dataset is.\n",
    "voc_dir = \"C:\\\\Users\\\\Gengqian Yang\\\\dataset\\\\VOC2012\"\n",
    "#choose the number of training examples (RAM consuming!!! 5000 images will occupy 16G of RAM)\n",
    "max_num = 5000\n",
    "#max_num =None\n",
    "num_workers = 0\n",
    "#activate the GPU acceleration (otherwise training will be 10 times slower!)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#training function\n",
    "def train_model(model, criterion, optimizer, dataload, num_epochs):\n",
    "    min_loss = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        dt_size = len(dataload.dataset)\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for x, y in dataload:\n",
    "            step += 1\n",
    "            inputs = x.to(device)\n",
    "            labels = y.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(net.parameters(),0.1)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            print(\"%d/%d,train_loss:%0.3f\" % (step, (dt_size - 1) // dataload.batch_size + 1, loss.item()))\n",
    "        print(\"epoch %d loss:%0.3f\" % (epoch, epoch_loss/step))\n",
    "        if (epoch_loss/step<=min_loss):\n",
    "            min_loss = epoch_loss/step\n",
    "            torch.save(model.state_dict(), \"new modified network weights attempt.pth\")\n",
    "            print(\"saved\")\n",
    "    return model\n",
    "    \n",
    "\n",
    "def train(batch_size,num_epochs):\n",
    "    #activate this function to initialise a network at the first time\n",
    "    #model = network().apply(initialization).to(device)\n",
    "    model =network().to(device)\n",
    "    #carrying on the training by loading the weights saved by previous training\n",
    "    model.load_state_dict(torch.load(\"new modified network weights.pth\",map_location=torch.device('cpu')))\n",
    "    criterion = nn.MSELoss()\n",
    "    #optimizer = torch.optim.Adam(model.parameters())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    voc_train = DataSet(True, voc_dir, max_num)\n",
    "    train_iter = torch.utils.data.DataLoader(voc_train, batch_size=batch_size, shuffle=True,\n",
    "                              drop_last=True, num_workers=0)\n",
    "    print(device)\n",
    "    train_model(model, criterion, optimizer, train_iter, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this function to train the network\n",
    "train(batch_size=8, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620, 460)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### load a testing example\n",
    "img=Image.open('C:\\\\Users\\\\Gengqian Yang\\\\Desktop\\\\runtime test\\\\1.png')\n",
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer it into a tensor\n",
    "transform=torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input=tsf(img).unsqueeze(0)\n",
    "#add one dimension as the input of the network is (batch number, 3, h, w)\n",
    "input=transform(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#activate the evaluation mode to freeze the weight\n",
    "net.eval()\n",
    "#load the weights\n",
    "net.load_state_dict(torch.load(\"new modified network weights.pth\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=net(input)\n",
    "unloader = torchvision.transforms.ToPILImage()\n",
    "image = output  # clone the tensor\n",
    "image = image.squeeze(0)  # remove the fake batch dimension\n",
    "image = unloader(image)\n",
    "#check the image by PIL image library\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the result\n",
    "image.save('C:\\\\Users\\\\Gengqian Yang\\\\CNN results\\\\home 3 network dehazed.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
